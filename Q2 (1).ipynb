{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0a842a",
   "metadata": {},
   "source": [
    "# TASK I (Text-Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97571db",
   "metadata": {},
   "source": [
    "## Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5151453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ishit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Input, Conv1D, MaxPooling1D\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, Dropout, concatenate, LSTM, Conv2D, MaxPooling2D, Flatten, GlobalMaxPooling1D,  Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0111862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ddc16",
   "metadata": {},
   "source": [
    "##### Defining some important metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d1d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf263d53",
   "metadata": {},
   "source": [
    "## Load the training, testing and valdiation JSONLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44d6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_li = []\n",
    "# # Open the JSONL file\n",
    "# with open('./train.jsonl', 'r') as f:\n",
    "#     # Read each line of the file as a separate JSON object\n",
    "#     for line in f:\n",
    "#         train_li.append(json.loads(line))\n",
    "        \n",
    "# test_li = []\n",
    "# # Open the JSONL file\n",
    "# with open('./test_seen.jsonl', 'r') as f:\n",
    "#     # Read eactest_seen.jsonlh line of the file as a separate JSON object\n",
    "#     for line in f:\n",
    "#         test_li.append(json.loads(line))\n",
    "        \n",
    "# dev_li = []\n",
    "# # Open the JSONL file\n",
    "# with open('./dev_seen.jsonl', 'r') as f:\n",
    "#     # Read eactest_seen.jsonlh line of the file as a separate JSON object\n",
    "#     for line in f:\n",
    "#         dev_li.append(json.loads(line))\n",
    "        \n",
    "# ## Getting dataframes\n",
    "train_path = \"C:/Users/Ishit/pytorch/DL_A4/data/df_train_cleaned.csv\"\n",
    "val_path = \"C:/Users/Ishit/pytorch/DL_A4/data/df_dev_cleaned.csv\"\n",
    "test_path = \"C:/Users/Ishit/pytorch/DL_A4/data/df_train_cleaned.csv\"\n",
    "image_path = \"C:/Users/Ishit/Downloads/archive (1)/data/\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "dev_df = pd.read_csv(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946f716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df['text'].tolist()\n",
    "test_text = test_df['text'].tolist()\n",
    "dev_text = dev_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57582f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label'].tolist()\n",
    "y_test = test_df['label'].tolist()\n",
    "y_dev = dev_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b016e5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  8388\n"
     ]
    }
   ],
   "source": [
    "# unique words\n",
    "unique_words = set()\n",
    "for sent in train_text:\n",
    "    for word in sent.split():\n",
    "        unique_words.add(word)\n",
    "\n",
    "print(\"Number of unique words: \", len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad0d9a",
   "metadata": {},
   "source": [
    "### Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7f123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6452 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data\n",
    "\n",
    "# max_words = 12000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(train_df['text'])\n",
    "sequences_val = tokenizer.texts_to_sequences(dev_df['text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442abdda",
   "metadata": {},
   "source": [
    "### Getting final train, test, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1705a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(sequences_train, maxlen=max_len)\n",
    "x_val = pad_sequences(sequences_val, maxlen=max_len)\n",
    "x_test = pad_sequences(sequences_test, maxlen=max_len)\n",
    "\n",
    "y_train = to_categorical(train_df['label'])\n",
    "y_val = to_categorical(dev_df['label'])\n",
    "y_test = to_categorical(test_df['label'])\n",
    "\n",
    "# Preprocess the labels\n",
    "y_train = np.array(train_df['label'])\n",
    "y_val = np.array(dev_df['label'])\n",
    "y_test = np.array(test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7987fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextClassification(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb799207",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextClassification(x_train, y_train)\n",
    "val_dataset = TextClassification(x_val, y_val)\n",
    "test_dataset = TextClassification(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b3bbe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "glove_model = gensim.models.KeyedVectors.load(\"glove_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb947df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the model using LSTM in pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7262886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a03936e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.lstm(out, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f6355ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "\n",
    "model = LSTM(torch.FloatTensor(glove_model.vectors), hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1323aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6f88aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 84.91422459483147\n",
      "Epoch [2], Loss: 83.66076129674911\n",
      "Epoch [3], Loss: 82.17684003710747\n",
      "Epoch [4], Loss: 80.42768996953964\n",
      "Epoch [5], Loss: 77.32658964395523\n",
      "Epoch [6], Loss: 73.38861733675003\n",
      "Epoch [7], Loss: 68.79930329322815\n",
      "Epoch [8], Loss: 62.21498078107834\n",
      "Epoch [9], Loss: 55.97354832291603\n",
      "Epoch [10], Loss: 49.6116042137146\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}], Loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cc98b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_lstm_model_1():\n",
    "    text_input = Input(shape=(max_len,))\n",
    "\n",
    "    text_embedding = Embedding(max_words, 128)(text_input)\n",
    "\n",
    "    text_lstm = LSTM(64)(text_embedding)\n",
    "\n",
    "    dense1 = Dense(64, activation='relu')(text_lstm)\n",
    "    dropout1 = Dropout(0.2)(dense1)\n",
    "\n",
    "    dense2 = Dense(32, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.2)(dense2)\n",
    "\n",
    "    dense3 = Dense(8, activation='relu')(dropout2)\n",
    "    dropout3 = Dropout(0.2)(dense3)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dropout3)\n",
    "\n",
    "    model = Model(inputs = text_input, outputs = output)\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy', 'Precision', 'Recall', f1_m])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f20d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def return_lstm_model_2():\n",
    "    text_input = Input(shape=(max_len,))\n",
    "\n",
    "    text_embedding = Embedding(max_words, 128)(text_input)\n",
    "\n",
    "    text_lstm_1 = LSTM(128, return_sequences=True)(text_embedding)\n",
    "    text_lstm_2 = LSTM(64)(text_lstm_1)\n",
    "\n",
    "    dense1 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(text_lstm_2)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "\n",
    "    dense2 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(dropout1)\n",
    "    dropout2 = Dropout(0.5)(dense2)\n",
    "\n",
    "    dense3 = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01))(dropout2)\n",
    "    dropout3 = Dropout(0.5)(dense3)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dropout3)\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=output)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001), metrics=['accuracy', 'Precision', 'Recall', f1_m])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ae36b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_lstm_model_3():\n",
    "    text_input = Input(shape=(max_len,))\n",
    "\n",
    "    text_embedding = Embedding(max_words, 128)(text_input)\n",
    "\n",
    "    conv1 = Conv1D(64, 3, activation='relu')(text_embedding)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "\n",
    "    text_lstm = LSTM(64)(pool1)\n",
    "\n",
    "    dense1 = Dense(64, activation='relu')(text_lstm)\n",
    "    dropout1 = Dropout(0.2)(dense1)\n",
    "\n",
    "    dense2 = Dense(32, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.2)(dense2)\n",
    "\n",
    "    dense3 = Dense(8, activation='relu')(dropout2)\n",
    "    dropout3 = Dropout(0.2)(dense3)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dropout3)\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=output)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy', 'Precision', 'Recall', f1_m])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ed21aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_lstm_model_4():\n",
    "    text_input = Input(shape=(max_len,))\n",
    "\n",
    "    # Load pre-trained word embeddings\n",
    "    embeddings_index = {}\n",
    "    with open('C:/Users/Ishit/pytorch/DL_A4/data/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    # Create embedding matrix\n",
    "    embedding_dim = 100\n",
    "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_words:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # Create embedding layer\n",
    "    text_embedding = Embedding(max_words, embedding_dim, weights=[embedding_matrix], trainable=False)(text_input)\n",
    "\n",
    "    text_lstm = LSTM(64)(text_embedding)\n",
    "\n",
    "    dense1 = Dense(64, activation='relu')(text_lstm)\n",
    "    dropout1 = Dropout(0.2)(dense1)\n",
    "\n",
    "    dense2 = Dense(32, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.2)(dense2)\n",
    "\n",
    "    dense3 = Dense(8, activation='relu')(dropout2)\n",
    "    dropout3 = Dropout(0.2)(dense3)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid')(dropout3)\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=output)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy', 'Precision', 'Recall', f1_m])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0ff7e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31cbd156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \n",
    "    # Plot the training and validation loss\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the training and validation accuracy\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model):\n",
    "    \n",
    "    test_loss, test_acc, test_precision, test_recall, test_f1 = model.evaluate(x_test, y_test)\n",
    "    train_loss, train_acc, train_precision, train_recall, train_f1 = model.evaluate(x_train, y_train)\n",
    "    dev_loss, dev_acc, dev_precision, dev_recall, dev_f1 = model.evaluate(x_val, y_val)\n",
    "    \n",
    "    \n",
    "    print(\"#######################################################################################################\")\n",
    "    print(\"#######################################################################################################\")\n",
    "    \n",
    "    print(\"Following are the TEST metrics associated with the model\\n\")    \n",
    "    print(f'Test loss: {test_loss}\\n')\n",
    "    print(f'Test acuuracy: {test_acc}\\n')\n",
    "    print(f'Test precision: {test_precision}\\n')\n",
    "    print(f'Test recall: {test_recall}\\n')\n",
    "    print(f'Test F1-Score: {test_f1}\\n')\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(classification_report(y_test.reshape(-1,1), y_pred.round()))\n",
    "    \n",
    "    print(\"#######################################################################################################\")\n",
    "    print(\"#######################################################################################################\")\n",
    "    print(\"Following are the TRAIN metrics associated with the model\\n\")    \n",
    "    print(f'Train loss: {train_loss}\\n')\n",
    "    print(f'Train acuuracy: {train_acc}\\n')\n",
    "    print(f'Train precision: {train_precision}\\n')\n",
    "    print(f'Train recall: {train_recall}\\n')\n",
    "    print(f'Train F1-Score: {train_f1}\\n')\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    print(classification_report(y_train.reshape(-1,1), y_pred_train.round()))\n",
    "    \n",
    "    print(\"#######################################################################################################\")\n",
    "    print(\"#######################################################################################################\")\n",
    "    print(\"Following are the DEV metrics associated with the model\\n\")    \n",
    "    print(f'Train loss: {dev_loss}\\n')\n",
    "    print(f'Train acuuracy: {dev_acc}\\n')\n",
    "    print(f'Train precision: {dev_precision}\\n')\n",
    "    print(f'Train recall: {dev_recall}\\n')\n",
    "    print(f'Train F1-Score: {dev_f1}\\n')\n",
    "    y_pred_dev = model.predict(x_val)\n",
    "    print(classification_report(y_val.reshape(-1,1), y_pred_dev.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e8c70",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a50d59ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm_model_4 = return_lstm_model_4()\n",
    "# history_lstm_4 = lstm_model_4.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5e93108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_metrics(lstm_model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32e7e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_history(history_lstm_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb657859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
